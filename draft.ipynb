{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json ,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fls = os.listdir('data/ObliQADataset/StructuredRegulatoryDocuments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = {}\n",
    "for f in fls:\n",
    "\tfile = json.load(open('data/ObliQADataset/StructuredRegulatoryDocuments/'+f))\n",
    "\tc = f[:-5]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/ObliQA_test.json'))\n",
    "dc = {}\n",
    "for q in test:\n",
    "\tdc[len(q['Passages'])] = dc.get(len(q['Passages']),0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2126, 2: 506, 4: 36, 3: 105, 6: 4, 5: 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  3\n",
       "1  2  2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'a':[1,2], 'b':[3,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Corpus ...\n"
     ]
    }
   ],
   "source": [
    "c, corpus, idx = 0, {}, {}\n",
    "print('Building Corpus ...')\n",
    "for i in range(1,41):\n",
    "\tfile = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/StructuredRegulatoryDocuments/'+f'{i}.json'))\n",
    "\tfor p in file:\n",
    "\t\tdid, pid = p['DocumentID'] ,p['PassageID']\n",
    "\t\tif f'{did}_{pid}' in idx: print(f'{did}_{pid}')\n",
    "\t\tidx[f'{did}_{pid}'] = c\n",
    "\t\tcorpus[c] = {'id':c, 'doc':did, 'psg':pid, 'txt':p['Passage']}\n",
    "\t\tc+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(corpus, open('/home/ubuntu/projects/regnlp/data/ObliQADataset/corpus.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/projects/regnlp/data/ObliQADataset/corpus.txt','w') as fh:\n",
    "\tfor l in corpus:\n",
    "\t\tfh.write(f'{l}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "revidx = {v:k for k,v in idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(revidx, open('/home/ubuntu/projects/regnlp/data/ObliQADataset/index.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbls = {}\n",
    "for k in idx:\n",
    "\tif k.count('_') >= 2 and k[-2]=='_' and k[-1] in '1234567':\n",
    "\t\tdid, pid, vid = k.split('_')\n",
    "\t\tdbls[f'{did}_{pid}'] = dbls.get(f'{did}_{pid}',[f'{did}_{pid}']) + [k]\n",
    "\n",
    "for k in dbls:\n",
    "\tvs = dbls[k]\n",
    "\tfile = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/StructuredRegulatoryDocuments/'+f'{k.split(\"_\")[0]}.json'))\n",
    "\tpss = [{'id':v, 'txt':p['Passage']} for v in vs for p in file if p['PassageID']=='_'.join(v.split('_')[1:])]\n",
    "\tdbls[k] = pss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7_3.3.40.Guidance': [{'id': '7_3.3.40.Guidance', 'txt': ''},\n",
       "  {'id': '7_3.3.40.Guidance_1',\n",
       "   'txt': 'An Authorised Person should observe best practices when establishing the systems and controls required under Rule 3.3.38, given the evolving nature of Financial Crime. Such practices may include, but are not limited to, incorporating secure authentication, biometrics and the monitoring of customer behaviour metrics into anti-fraud and anti-Financial Crime systems and controls.\\n\\n'}],\n",
       " '7_1.': [{'id': '7_1.', 'txt': 'INTRODUCTION\\n'},\n",
       "  {'id': '7_1._1',\n",
       "   'txt': 'Corporate governance is a framework of systems, policies, procedures and controls through which an entity:\\na.\\tpromotes the sound and prudent management of its business;\\nb.\\tprotects the interests of its Customers and stakeholders; and\\nc.\\tplaces clear responsibility for achieving Rule \\u200e3.3.41(2)(a) and \\u200e(3) on the Governing Body and its members and the senior management of the Authorised Person.\\n'}],\n",
       " '7_5.2.13': [{'id': '7_5.2.13', 'txt': ''},\n",
       "  {'id': '7_5.2.13_1',\n",
       "   'txt': '(1) \\tIn assessing an application for a Financial Services Permission the Regulator may, by means of written notice, indicate the legal form that the applicant may adopt to enable authorisation to be granted.'},\n",
       "  {'id': '7_5.2.13_2',\n",
       "   'txt': '(2)\\tWhere the Regulator thinks it appropriate it may treat an application made by one legal form or Person as having been made by the new legal form or Person.'}],\n",
       " '7_8.4.1.Guidance': [{'id': '7_8.4.1.Guidance',\n",
       "   'txt': 'When considering a withdrawal of a Financial Services Permission, the Regulator takes into account a number of matters including those outlined in the GPM.'},\n",
       "  {'id': '7_8.4.1.Guidance_1',\n",
       "   'txt': 'This Rule addresses applications or requests regarding Approved Persons with respect to Parts 3, 4 and 5 of the FSMR.'}],\n",
       " '11_2.3.16.Guidance': [{'id': '11_2.3.16.Guidance',\n",
       "   'txt': '\\nWhen deciding if an Applicant’s total tangible assets are in a form readily convertible to cash, the Regulator would not normally consider inventories or receivables as being readily convertible to cash.'},\n",
       "  {'id': '11_2.3.16.Guidance_1', 'txt': ''}],\n",
       " '11_1.': [{'id': '11_1.', 'txt': 'INTRODUCTION'},\n",
       "  {'id': '11_1._1',\n",
       "   'txt': 'the Listed Entity has failed to meet its continuing obligations for admission to the Official List;'},\n",
       "  {'id': '11_1._2',\n",
       "   'txt': 'the requirements in Rule 10.1.8 pertaining to a Reporting Entity are to be interpreted as applying to a Listed Fund in place of a Listed Entity; and'},\n",
       "  {'id': '11_1._3',\n",
       "   'txt': 'the requirements in Rule 7.5 pertaining to a Reporting Entity are to be interpreted as applying to a Listed Fund in place of a Listed Entity; and'}],\n",
       " '11_2.': [{'id': '11_2.', 'txt': 'THE LISTING RULES'},\n",
       "  {'id': '11_2._1',\n",
       "   'txt': 'the Listed Entity has failed to Disclose financial information in accordance with these Rules;'},\n",
       "  {'id': '11_2._2',\n",
       "   'txt': 'the reference in Rule 10.1.8(3)(b) to Shares is to be interpreted as applying to Units of a Listed Fund.'},\n",
       "  {'id': '11_2._3',\n",
       "   'txt': 'references in Rule 7.5 to a Listed Entity’s Securities are to be read as applying to Units of a Listed Fund.\\n\\n'}],\n",
       " '11_3.': [{'id': '11_3.',\n",
       "   'txt': 'the Listed Entity is unable to assess accurately its financial position and inform the market accordingly;'},\n",
       "  {'id': '11_3._1', 'txt': 'LISTED FUNDS'}],\n",
       " '11_4.': [{'id': '11_4.',\n",
       "   'txt': 'there is insufficient publicly available information in the market about a proposed transaction which involves the Listed Entity or the Relevant Securities;'},\n",
       "  {'id': '11_4._1', 'txt': 'OFFERS OF SECURITIES'}],\n",
       " '11_5.': [{'id': '11_5.',\n",
       "   'txt': \"the Listed Entity's Securities have been suspended elsewhere;\"},\n",
       "  {'id': '11_5._1', 'txt': 'SPONSORS AND COMPLIANCE ADVISERS'}],\n",
       " '11_6.': [{'id': '11_6.',\n",
       "   'txt': 'the Listed Entity has appointed Administrators or receivers, or is an Investment Trust or Fund and is winding up;'},\n",
       "  {'id': '11_6._1',\n",
       "   'txt': 'MARKET ABUSE, PRICE STABILISATION AND BUY-BACK PROGRAMMES'}],\n",
       " '11_6.2.9.Guidance': [{'id': '11_6.2.9.Guidance',\n",
       "   'txt': '\\nTo be adequately disclosed in a Prospectus, the information should appear under its own separate heading in the first few pages of the Prospectus.\\n'},\n",
       "  {'id': '11_6.2.9.Guidance_1',\n",
       "   'txt': '\\n\\nRule 6.2.9(3) requires a Stabilisation Manager to disclose to the Regulator details of each Price Stabilisation transaction conducted during the Stabilisation Window. The purpose of this Rule is to provide the Regulator with an understanding of the price support afforded the Relevant Securities during the Stabilisation Window and the manner in which Price Stabilisation occurred.\\n'}],\n",
       " '11_6.2.17.Guidance': [{'id': '11_6.2.17.Guidance',\n",
       "   'txt': '\\n‘Dual-listed Relevant Securities’ in (2) would, in relation to one Listed Security, include Certificates (e.g., global depository receipts) and Warrants over the other Listed Security.\\n'},\n",
       "  {'id': '11_6.2.17.Guidance_1',\n",
       "   'txt': '\\nRule 6.2.17 allows a Person who is acting as a Stabilisation Manager in respect of a dual-listing of Relevant Securities to rely on the Price Stabilisation Rules or on the laws of a Zone 1 jurisdiction to conduct those activities.  The Rule is intended to provide Stabilisation Managers with some limited flexibility in respect of their activities in the ADGM, so long as those activities are appropriately regulated.'}],\n",
       " '11_7.': [{'id': '11_7.',\n",
       "   'txt': 'the Relevant Securities are a securitised Derivative and any underlying Instrument is suspended; or'},\n",
       "  {'id': '11_7._1', 'txt': 'CONTINUOUS DISCLOSURE'}],\n",
       " '11_8.': [{'id': '11_8.',\n",
       "   'txt': 'for a Derivative which carries a right to buy or subscribe for another Security, the Security over which the Derivative carries a right to buy or subscribe has been suspended.'},\n",
       "  {'id': '11_8._1', 'txt': 'SYSTEMS AND CONTROLS'}],\n",
       " '13_APP2.A2.1.2': [{'id': '13_APP2.A2.1.2',\n",
       "   'txt': 'Value of Trading Book positions'},\n",
       "  {'id': '13_APP2.A2.1.2_1',\n",
       "   'txt': 'Positions included in the Trading Book. An Authorised Person must include in its Trading Book, subject to the Rules on trading intent and hedging Non Trading Book Exposures:\\n(a)\\teach proprietary position in a Financial Instrument, commodity or commodity Derivative which is held with trading intent as detailed in Rule A2.1.5(3);\\n(b)\\teach position arising from Matched Principal broking and market making;\\n(c)\\teach position taken in order to hedge another element of the Trading Book;\\n(d)\\teach Exposure due to a repurchase agreement (repo), or Securities and commodities lending, which is based on a Security or commodity included in the Trading Book;\\n(e)\\teach Exposure due to a reverse repurchase agreement (reverse repo), or Securities and commodities borrowing transaction included in the Trading Book;\\n(f)\\teach Exposure arising from an Unsettled Transaction, free delivery or OTC Derivative; and\\n(g)\\teach Exposure in the form of a fee, commission, interest, dividend or margin on an exchange traded Derivative directly related to the items included in the Trading Book.'}],\n",
       " '17_Part 12.Chapter 1.134.': [{'id': '17_Part 12.Chapter 1.134.',\n",
       "   'txt': 'Supervision. Revoking recognition'},\n",
       "  {'id': '17_Part 12.Chapter 1.134._1',\n",
       "   'txt': '(1)\\tA Recognition Order in respect of a Recognised Body or in respect of a Remote Body may be revoked by an order made by the Regulator at the request, or with the consent, of the Recognised Body or Remote Body concerned.'},\n",
       "  {'id': '17_Part 12.Chapter 1.134._2',\n",
       "   'txt': '(2)\\tIf it appears to the Regulator that a Recognised Body or Remote Body—\\n(a)\\tis failing, or has failed, to satisfy the Recognition Requirements or Remote Recognition Requirements, as applicable; or\\n(b)\\tis failing, or has failed, to comply with any other obligation imposed on it by or under these Regulations;\\nit may make an order revoking the Recognition Order for that body.'},\n",
       "  {'id': '17_Part 12.Chapter 1.134._3',\n",
       "   'txt': '(3)\\tIf it appears to the Regulator that a Recognised Body—\\n(a)\\thas not carried on the business of an investment exchange or (as the case may be) of a clearing house during the period of 12 months beginning with the day on which the Recognition Order took effect in relation to it; or\\n(b)\\thas not carried on the business of an investment exchange or (as the case may be) of a clearing house at any time during the period of six months ending with the Relevant Day;\\nit may make an order revoking the Recognition Order for that body.'},\n",
       "  {'id': '17_Part 12.Chapter 1.134._4',\n",
       "   'txt': '(4)\\tAn order under this section (a \"Revocation Order\") must specify the date on which it is to take effect.'},\n",
       "  {'id': '17_Part 12.Chapter 1.134._5',\n",
       "   'txt': '(5)\\tIn the case of a Revocation Order made under subsection \\u200e(2) or \\u200e(3), the specified date must not be earlier than the end of the period of three months beginning with the day on which the order is made.'},\n",
       "  {'id': '17_Part 12.Chapter 1.134._6',\n",
       "   'txt': '(6)\\tA Revocation Order may contain such transitional provisions as the Regulator thinks necessary or expedient.'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc =0\n",
    "for name in ['dev', 'test', 'train']:\n",
    "\tfile = json.load(open(f'/home/ubuntu/projects/regnlp/data/ObliQADataset/ObliQA_{name}.json'))\n",
    "\tfor i,q in enumerate(file):\n",
    "\t\tfor j,p in enumerate(q['Passages']):\n",
    "\t\t\tif f'{p[\"DocumentID\"]}_{p[\"PassageID\"]}' in dbls:\n",
    "\t\t\t\tfnd = False\n",
    "\t\t\t\tfor d in dbls[f'{p[\"DocumentID\"]}_{p[\"PassageID\"]}']:\n",
    "\t\t\t\t\tif d['txt'] == p['Passage']:\n",
    "\t\t\t\t\t\tfnd = True \n",
    "\t\t\t\t\t\tfile[i]['Passages'][j]['PassageID'] = d['id']\n",
    "\t\t\t\t\t\tcc +=1\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif not fnd : print(f'not found for {name}, {q}, {p}')\n",
    "\n",
    "\tjson.dump(file, open(f'/home/ubuntu/projects/regnlp/data/ObliQADataset/splits/ObliQA_{name}.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['dev', 'test', 'train']:\n",
    "\tfile = json.load(open(f'/home/ubuntu/projects/regnlp/data/ObliQADataset/splits/ObliQA_{name}.json'))\n",
    "\tout = []\n",
    "\tfor q in file:\n",
    "\t\t#ky = f'{q[\"DocumentID\"]}_{q[\"PassageID\"]}'\n",
    "\t\trefs = [idx[f'{p[\"DocumentID\"]}_{p[\"PassageID\"]}'] for p in q[\"Passages\"]]\n",
    "\t\tout.append({'query':q['Question'], 'pos':refs})\n",
    "\tjson.dump(out, open(f'/home/ubuntu/projects/regnlp/data/ObliQADataset/splits/{name}.json','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_14.2.3.Guidance.10.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revidx[482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13732"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/corpus.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in corpus.items():\n",
    "\tnm = f'{v[\"doc\"]}_{v[\"psg\"]}'\n",
    "\tnm = nm.split('/')[0]\n",
    "\twith open(f'/home/ubuntu/projects/regnlp/data/ObliQADataset/txt/{nm}.txt','w') as fh:\n",
    "\t\tfh.write(v[\"txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/new/index.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "revidx = {v:k for k,v in indx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_1.',\n",
       " '1_1.1',\n",
       " '1_1.1.1',\n",
       " '1_1.1.1.(1)',\n",
       " '1_1.1.1.(2)',\n",
       " '1_1.2',\n",
       " '1_1.2.1',\n",
       " '1_1.2.1.(1)',\n",
       " '1_1.2.1.(2)',\n",
       " '1_1.2.1.Guidance']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(revidx.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrevidx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "revidx['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_14.2.3.Guidance.10.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx['482']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus, tokenizer=None):\n",
    "        self.corpus_size = 0\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "        nd = self._initialize(corpus)\n",
    "        print(nd)\n",
    "        self._calc_idf(nd)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in frequencies.items():\n",
    "                try:\n",
    "                    nd[word]+=1\n",
    "                except KeyError:\n",
    "                    nd[word] = 1\n",
    "\n",
    "            self.corpus_size += 1\n",
    "\n",
    "        self.avgdl = num_doc / self.corpus_size\n",
    "        return nd\n",
    "\n",
    "\n",
    "class BM25Okapi(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        \"\"\"\n",
    "        Calculates frequencies of terms in documents and in corpus.\n",
    "        This algorithm sets a floor on the idf values to eps * average_idf\n",
    "        \"\"\"\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "        negative_idfs = []\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = idf_sum / len(self.idf)\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        \"\"\"\n",
    "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
    "        this algorithm also adds a floor to the idf value of epsilon.\n",
    "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusss = [\n",
    "\n",
    "\t\"i saw a cat\",\n",
    "\t\"lion is the best animal\",\n",
    "\t\"cat and dog are animal\",\n",
    "\t\"i have the best dog\",\n",
    "\t\"dog dog\"\n",
    "]\n",
    "\n",
    "corpus = [c.split() for c in corpusss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 0\n",
    "avgdl = 0\n",
    "doc_freqs = []\n",
    "idf = {}\n",
    "doc_len = []\n",
    "nd = {}  # word -> number of documents with word\n",
    "num_doc = 0\n",
    "for document in corpus:\n",
    "\tdoc_len.append(len(document))\n",
    "\tnum_doc += len(document)\n",
    "\n",
    "\tfrequencies = {}\n",
    "\tfor word in document:\n",
    "\t\tif word not in frequencies:\n",
    "\t\t\tfrequencies[word] = 0\n",
    "\t\tfrequencies[word] += 1\n",
    "\tdoc_freqs.append(frequencies)\n",
    "\n",
    "\tfor word, freq in frequencies.items():\n",
    "\t\ttry:\n",
    "\t\t\tnd[word]+=1\n",
    "\t\texcept KeyError:\n",
    "\t\t\tnd[word] = 1\n",
    "\n",
    "\tcorpus_size += 1\n",
    "\n",
    "avgdl = num_doc / corpus_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "idff_sum = 0\n",
    "epsilon = .25\n",
    "# collect words with negative idf to set them a special epsilon value.\n",
    "# idf can be negative if word is contained in more than half of documents\n",
    "negative_idfs = []\n",
    "for word, freq in nd.items():\n",
    "\tidff = math.log(corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\tidf[word] = idff\n",
    "\tidff_sum += idff\n",
    "\tif idff < 0:\n",
    "\t\tnegative_idfs.append(word)\n",
    "average_idf = idff_sum / len(idf)\n",
    "\n",
    "eps = epsilon * average_idf\n",
    "for word in negative_idfs:\n",
    "\tidf[word] = eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'i': 1, 'saw': 1, 'a': 1, 'cat': 1},\n",
       " {'lion': 1, 'is': 1, 'the': 1, 'best': 1, 'animal': 1},\n",
       " {'cat': 1, 'and': 1, 'dog': 1, 'are': 1, 'animal': 1},\n",
       " {'i': 1, 'have': 1, 'the': 1, 'best': 1, 'dog': 1},\n",
       " {'dog': 2}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'saw': 1,\n",
       " 'a': 1,\n",
       " 'cat': 2,\n",
       " 'lion': 1,\n",
       " 'is': 1,\n",
       " 'the': 2,\n",
       " 'best': 2,\n",
       " 'animal': 2,\n",
       " 'and': 1,\n",
       " 'dog': 3,\n",
       " 'are': 1,\n",
       " 'have': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = list(nd.keys())\n",
    "tok2idx = {voc[i]:i for i in range(len(voc))}\n",
    "idx2tok = {i:voc[i] for i in range(len(voc))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corp = [[tok2idx[t] for t in c] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [4, 5, 6, 7, 8], [3, 9, 10, 11, 8], [0, 12, 6, 7, 10], [10, 10]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for d in doc_freqs:\n",
    "\tdenc = [0]*13\n",
    "\tfor k,v in d.items():\n",
    "\t\tdenc[tok2idx[k]] = v\n",
    "\tdocs.append(denc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.33647223662121295,\n",
       " 'saw': 1.0986122886681098,\n",
       " 'a': 1.0986122886681098,\n",
       " 'cat': 0.33647223662121295,\n",
       " 'lion': 1.0986122886681098,\n",
       " 'is': 1.0986122886681098,\n",
       " 'the': 0.33647223662121295,\n",
       " 'best': 0.33647223662121295,\n",
       " 'animal': 0.33647223662121295,\n",
       " 'and': 1.0986122886681098,\n",
       " 'dog': 0.17377259552233887,\n",
       " 'are': 1.0986122886681098,\n",
       " 'have': 1.0986122886681098}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33647223662121295,\n",
       " 1.0986122886681098,\n",
       " 1.0986122886681098,\n",
       " 0.33647223662121295,\n",
       " 1.0986122886681098,\n",
       " 1.0986122886681098,\n",
       " 0.33647223662121295,\n",
       " 0.33647223662121295,\n",
       " 0.33647223662121295,\n",
       " 1.0986122886681098,\n",
       " 0.17377259552233887,\n",
       " 1.0986122886681098,\n",
       " 1.0986122886681098]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfvec = [0]*13\n",
    "for k,v in idf.items():\n",
    "\tidfvec[tok2idx[k]] = v\n",
    "idfvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k1, b = 1.5, .5\n",
    "query = [\"i\", \"have\", \"cat\", \"cat\"]\n",
    "score = np.zeros(corpus_size)\n",
    "doc_len = np.array(doc_len)\n",
    "for q in query:\n",
    "\tq_freq = np.array([(doc.get(q) or 0) for doc in doc_freqs])\n",
    "\tscore += (idf.get(q) or 0) * (q_freq * (k1 + 1) /\n",
    "\t\t\t\t\t\t\t\t\t\t(q_freq + k1 * (1 - b + b * doc_len / avgdl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.02404594, 0.        , 0.6365691 , 1.35751239, 0.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([1.0240, 0.0000, 0.6366, 1.3575, 0.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvec = [0]*13\n",
    "for q in query:\n",
    "\tqvec[tok2idx[q]] +=1\n",
    "qvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = torch.tensor(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = docs.sum(1, keepdim=True).repeat(1,docs.shape[-1]) /avgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9524, 0.9524, 0.9524, 0.9524, 0.9524, 0.9524, 0.9524, 0.9524, 0.9524,\n",
       "         0.9524, 0.9524, 0.9524, 0.9524],\n",
       "        [1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905,\n",
       "         1.1905, 1.1905, 1.1905, 1.1905],\n",
       "        [1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905,\n",
       "         1.1905, 1.1905, 1.1905, 1.1905],\n",
       "        [1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905, 1.1905,\n",
       "         1.1905, 1.1905, 1.1905, 1.1905],\n",
       "        [0.4762, 0.4762, 0.4762, 0.4762, 0.4762, 0.4762, 0.4762, 0.4762, 0.4762,\n",
       "         0.4762, 0.4762, 0.4762, 0.4762]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfvec = torch.tensor(idfvec)\n",
    "denom = (docs + k1 *(1 - b + b * doc_len))\n",
    "nom = idfvec * ((k1 + 1) * docs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_len = docs.sum(1, keepdim=True).repeat(1,docs.shape[-1]) /avgdl\n",
    "docs_idf =  nom/denom \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3413, 1.1145, 1.1145, 0.3413, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0392, 1.0392, 0.3183, 0.3183, 0.3183,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.3183, 0.0000, 0.0000, 0.0000, 0.0000, 0.3183,\n",
       "         1.0392, 0.1644, 1.0392, 0.0000],\n",
       "        [0.3183, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3183, 0.3183, 0.0000,\n",
       "         0.0000, 0.1644, 0.0000, 1.0392],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.2796, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = torch.tensor(qvec, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "scores = torch.matmul(docs_idf, qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240, 0.0000, 0.6366, 1.3575, 0.0000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.30757724, 0.30757724, 0.21342094, 0.41830505, 0.24320061,\n",
    "       3.31848878, 0.28263855, 1.32300047, 0.        , 0.45967588]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 7, 'saw': 3, 'the': 5, 'dog': 7, 'and': 3, 'are': 2, 'more': 1, 'than': 1, 'i': 2, 'a': 1, 'lion': 3, 'is': 1, 'best': 2, 'animal': 2, 'have': 1, 'ate': 1}\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25Okapi(cpr_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0.2614406550238335,\n",
       " 'saw': 0.7621400520468966,\n",
       " 'the': 0.0,\n",
       " 'dog': 0.2614406550238335,\n",
       " 'and': 0.7621400520468966,\n",
       " 'are': 1.2237754316221157,\n",
       " 'more': 1.845826690498331,\n",
       " 'than': 1.845826690498331,\n",
       " 'i': 1.2237754316221157,\n",
       " 'a': 1.845826690498331,\n",
       " 'lion': 0.7621400520468966,\n",
       " 'is': 1.845826690498331,\n",
       " 'best': 1.2237754316221157,\n",
       " 'animal': 1.2237754316221157,\n",
       " 'have': 1.845826690498331,\n",
       " 'ate': 1.845826690498331}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30757724, 0.30757724, 0.21342094, 0.41830505, 0.24320061,\n",
       "       3.31848878, 0.28263855, 1.32300047, 0.        , 0.45967588])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_scores([\"what\", \"is\", \"the\", \"best\", \"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51178984, 1.51178984, 0.60074043, 0.41830505, 0.9521681 ,\n",
       "       0.        , 0.56527709, 0.28263855, 0.30757724, 0.91935175])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_scores(cpr_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 3.0135946 , 0.        , 2.50281585,\n",
       "       5.96448971, 2.64600093, 1.32300047, 0.        , 0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_scores([\"best\", \"animal\", \"is\", \"more\", \"than\", \"a\", \"animal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.89160197, 6.89160197, 0.60074043, 0.41830505, 5.20597304,\n",
       "       0.        , 0.56527709, 0.28263855, 0.30757724, 0.91935175])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_scores(['cat', 'saw', 'the', 'dog', \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cat': 1, 'saw': 1, 'the': 1, 'dog': 1},\n",
       " {'dog': 1, 'saw': 1, 'the': 1, 'cat': 1},\n",
       " {'dog': 3, 'and': 2, 'are': 1, 'more': 1, 'than': 1, 'cat': 1},\n",
       " {'cat': 1},\n",
       " {'i': 1, 'saw': 1, 'a': 2, 'cat': 1, 'and': 1, 'lion': 1},\n",
       " {'lion': 1, 'is': 1, 'the': 1, 'best': 1, 'animal': 1},\n",
       " {'cat': 1, 'and': 1, 'dog': 1, 'are': 1, 'animal': 1},\n",
       " {'i': 1, 'have': 1, 'the': 1, 'best': 1, 'dog': 1},\n",
       " {'lion': 1, 'ate': 1, 'the': 1, 'dog': 1},\n",
       " {'cat': 8, 'dog': 8}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "a = torch.tensor(2., requires_grad=True)\n",
    "b = 3\n",
    "b = b + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tok = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.encode('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = tok.encode(\"i am a bad person\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  72,  716,  257, 2089, 1048]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dd)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = 5\n",
    "out = torch.zeros((1,voc_size), dtype=torch.uint8)\n",
    "out.index_fill(1, torch.tensor([0,3]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((1,4))\n",
    "b= torch.zeros((1,4))\n",
    "torch.cat([a,b]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = docs.sum(1, keepdim=True).repeat(1,docs.shape[-1]) /avgdl\n",
    "docs_idf = idf * ((k1 + 1) * docs ) / (docs + k1 *(1 - b + b * doc_len))\n",
    "scores = queries * docs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rankbm25_ml import BM25Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BM25Tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1',\n",
       "  Parameter containing:\n",
       "  tensor(1., device='cuda:0', requires_grad=True)),\n",
       " ('b',\n",
       "  Parameter containing:\n",
       "  tensor(2., device='cuda:0', requires_grad=True)),\n",
       " ('epsilon',\n",
       "  Parameter containing:\n",
       "  tensor(0.2000, device='cuda:0', requires_grad=True)),\n",
       " ('avgdl_bias',\n",
       "  Parameter containing:\n",
       "  tensor(0., device='cuda:0', requires_grad=True)),\n",
       " ('idf_bias',\n",
       "  Parameter containing:\n",
       "  tensor(0.5000, device='cuda:0', requires_grad=True))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM25Tune()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/recht/lib/python3.12/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AdamW\n",
    "opt = AdamW(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'output/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "mdlft = torch.load('/home/ubuntu/projects/regnlp/output/bm25_idft.pt', weights_only=True)\n",
    "mdlbs = torch.load('/home/ubuntu/projects/regnlp/output/bm25.pt', weights_only=True)\n",
    "mdlftrnd = torch.load('/home/ubuntu/projects/regnlp/output/bm25_idft_rnd.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame({'base':mdlbs['idf'].numpy(), 'ftune':mdlft['idf'].numpy(), 'ftunernd':mdlftrnd['idf'].numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/idf_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.53131103515625e-05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df['base'] - df['ftune']).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlbs['k1'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "\n",
    "\n",
    "def convert2format(queries, corpus):\n",
    "\tnew_q, mapping = {}, {}\n",
    "\tnew_c = {k:v['txt'] for k,v in corpus.items()}\n",
    "\tfor i,q in enumerate(queries):\n",
    "\t\tnew_q[str(i)] = q['query']\n",
    "\t\tmapping[str(i)] = [str(p) for p in q['pos']]\n",
    "\treturn new_c, new_q, mapping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/new/corpus.json'))\n",
    "queries = json.load(open('/home/ubuntu/projects/regnlp/data/ObliQADataset/new/test.json'))\n",
    "new_c, new_q, mapping = convert2format(queries, corpus)\n",
    "\n",
    "model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "ir_evaluator = InformationRetrievalEvaluator(\n",
    "\tqueries=new_q,\n",
    "\tcorpus=new_c,\n",
    "\trelevant_docs=mapping,\n",
    "\tshow_progress_bar=True\n",
    ")\n",
    "results = ir_evaluator(model)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
